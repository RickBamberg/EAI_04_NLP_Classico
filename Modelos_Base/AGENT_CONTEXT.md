# AGENT_CONTEXT.md - Modelos Base de NLP Cl√°ssico

> **Prop√≥sito**: Contexto t√©cnico dos 3 notebooks de modelos prontos  
> **√öltima atualiza√ß√£o**: Janeiro 2026  
> **Tipo**: Templates de modelos para classifica√ß√£o de texto

## RESUMO EXECUTIVO

**Objetivo**: Fornecer modelos prontos e otimizados para classifica√ß√£o de texto  
**Notebooks**: 3 notebooks com c√≥digo de produ√ß√£o  
**Modelos**: Naive Bayes, SVM, compara√ß√µes  
**Performance**: 85-92% accuracy t√≠pico  
**Uso**: Copiar, adaptar, deployar  
**Diferencial**: C√≥digo completo testado, n√£o apenas conceitos

---

## NOTEBOOK 1: naive_bayes_sentimentos.ipynb

### Objetivo
Template completo de Naive Bayes para an√°lise de sentimento.

### Por Que Naive Bayes para NLP?

#### Vantagens Matem√°ticas
```
P(classe|documento) = P(documento|classe) √ó P(classe) / P(documento)

Naive Assumption: Features s√£o independentes
P(doc|classe) = P(palavra1|classe) √ó P(palavra2|classe) √ó ...

Isso simplifica MUITO o c√°lculo!
```

#### Vantagens Pr√°ticas
- ‚ö° **R√°pido**: Treino e predi√ß√£o em milissegundos
- üìä **Poucos dados**: Funciona com 100-1000 exemplos
- üéØ **Baseline forte**: 85-88% accuracy t√≠pico
- üîç **Interpret√°vel**: Pode ver probabilidades por palavra

### MultinomialNB - Espec√≠fico para Texto

```python
from sklearn.naive_bayes import MultinomialNB

clf = MultinomialNB(
    alpha=1.0,        # Laplace smoothing
    fit_prior=True    # Aprende probabilidade das classes
)
```

#### Alpha (Laplace Smoothing)
```python
# Problema: Palavra nunca vista
P("palavranova"|positivo) = 0 / total  # Divis√£o por zero!

# Solu√ß√£o: Adicionar alpha
P("palavranova"|positivo) = (0 + alpha) / (total + alpha*|V|)

# alpha=1.0: Smoothing padr√£o
# alpha=0.1: Menos smoothing (mais confiante)
# alpha=10.0: Mais smoothing (mais cauteloso)
```

### Pipeline Completo

```python
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# Dados
textos = ["Adorei!", "P√©ssimo!", ...]
labels = ["positivo", "negativo", ...]

# Split
X_train, X_test, y_train, y_test = train_test_split(
    textos, labels, test_size=0.2, random_state=42, stratify=labels
)

# Pipeline
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(
        max_features=5000,
        ngram_range=(1,1),  # Apenas unigramas (NB funciona bem assim)
        min_df=2
    )),
    ('nb', MultinomialNB(alpha=1.0))
])

# Treinar
pipeline.fit(X_train, y_train)

# Avaliar
y_pred = pipeline.predict(X_test)
print(classification_report(y_test, y_pred))

# Accuracy t√≠pica: 85-88%
```

### Interpretabilidade - Ver Features Importantes

```python
# Ap√≥s treinar
tfidf = pipeline.named_steps['tfidf']
nb = pipeline.named_steps['nb']

# Features (palavras)
feature_names = tfidf.get_feature_names_out()

# Log probabilidades por classe
log_probs = nb.feature_log_prob_

# Top 10 palavras por classe
for i, classe in enumerate(nb.classes_):
    top_indices = log_probs[i].argsort()[-10:][::-1]
    top_features = [feature_names[idx] for idx in top_indices]
    print(f"\nTop palavras para '{classe}':")
    print(top_features)
```

### Quando Usar Naive Bayes

**‚úÖ Use Naive Bayes quando**:
- Precisa de baseline r√°pido
- Tem poucos dados (<10k exemplos)
- Quer interpretabilidade
- Velocidade √© cr√≠tica
- Classifica√ß√£o de spam, sentimento simples

**‚ùå N√£o use Naive Bayes quando**:
- Features s√£o muito correlacionadas
- Precisa capturar intera√ß√µes complexas
- Accuracy <85% n√£o √© aceit√°vel
- Tem muitos dados (use SVM ou Deep Learning)

---

## NOTEBOOK 2: classificacao_texto_svm.ipynb

### Objetivo
Template de SVM para classifica√ß√£o multi-classe com alta performance.

### Por Que SVM para NLP?

#### Support Vector Machines - Conceito
```
Encontrar hiperplano que melhor separa as classes
maximizando a margem entre elas

     Classe A        |        Classe B
        ‚Ä¢            |            ‚ó¶
      ‚Ä¢   ‚Ä¢       MARGEM        ‚ó¶  ‚ó¶
        ‚Ä¢            |            ‚ó¶
```

#### Por Que Funciona Bem em Texto?
- ‚úÖ **Alta dimensionalidade**: 10k-100k features n√£o √© problema
- ‚úÖ **Espa√ßos esparsos**: TF-IDF √© esparso, SVM lida bem
- ‚úÖ **Margens claras**: Textos de classes diferentes geralmente bem separados
- ‚úÖ **Kernel trick**: Pode aprender rela√ß√µes n√£o-lineares

### LinearSVC - Otimizado para Texto

```python
from sklearn.svm import LinearSVC

clf = LinearSVC(
    C=1.0,              # Regulariza√ß√£o (inversa)
    max_iter=1000,      # Itera√ß√µes m√°ximas
    dual=False,         # Primal (se n_samples > n_features)
    random_state=42
)
```

#### Par√¢metro C (Regulariza√ß√£o)

```python
# C pequeno (0.1): Mais regulariza√ß√£o
# ‚Üí Margin maior, pode underfit
# ‚Üí Generaliza melhor, menos overfit

# C m√©dio (1.0): Padr√£o balanceado ‚úì

# C grande (10.0): Menos regulariza√ß√£o
# ‚Üí Margin menor, pode overfit
# ‚Üí Accuracy maior no treino
```

### Pipeline Completo

```python
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.model_selection import GridSearchCV

# Pipeline
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(
        max_features=10000,     # Mais features que NB
        ngram_range=(1, 2),     # Uni + Bigramas
        min_df=2,
        max_df=0.8,
        sublinear_tf=True       # log(TF)
    )),
    ('svm', LinearSVC(
        C=1.0,
        max_iter=1000,
        random_state=42
    ))
])

# Grid Search (opcional)
param_grid = {
    'tfidf__max_features': [5000, 10000],
    'tfidf__ngram_range': [(1,1), (1,2)],
    'svm__C': [0.1, 1.0, 10.0]
}

grid = GridSearchCV(
    pipeline,
    param_grid,
    cv=5,
    n_jobs=-1,
    verbose=1
)

# Treinar
grid.fit(X_train, y_train)

print(f"Melhores par√¢metros: {grid.best_params_}")
print(f"Melhor CV score: {grid.best_score_:.3f}")

# Accuracy t√≠pica: 89-92%
```

### Troubleshooting - Converg√™ncia

```python
# Problema: ConvergenceWarning
# "Objective did not converge"

# Solu√ß√£o 1: Aumentar itera√ß√µes
LinearSVC(max_iter=2000)

# Solu√ß√£o 2: Normalizar (j√° faz TF-IDF)
# TfidfVectorizer aplica L2 norm automaticamente

# Solu√ß√£o 3: Reduzir C
LinearSVC(C=0.1)
```

### SVM vs Naive Bayes

```python
# Experimento t√≠pico:
# Dataset: 10k reviews, 2 classes

Naive Bayes:
- Treino: ~1 segundo
- Predi√ß√£o: ~0.1 segundo
- Accuracy: 86%

LinearSVC:
- Treino: ~5 segundos
- Predi√ß√£o: ~0.1 segundo
- Accuracy: 91%

Conclus√£o: SVM vale o custo extra de treino para +5% accuracy
```

### Quando Usar SVM

**‚úÖ Use SVM quando**:
- Quer melhor accuracy (~90%+)
- Tem >1k exemplos
- Alta dimensionalidade (TF-IDF)
- Classifica√ß√£o multi-classe
- Produ√ß√£o (predi√ß√£o √© r√°pida)

**‚ùå N√£o use SVM quando**:
- Tem >1M exemplos (muito lento)
- Precisa de probabilidades calibradas (use LogisticRegression)
- Quer interpretabilidade m√°xima (use NB)

---

## NOTEBOOK 3: comparativo_tfidf_vs_embeddings.ipynb

### Objetivo
Compara√ß√£o emp√≠rica de 3 abordagens para classifica√ß√£o de texto.

### Experimento 1: TF-IDF + Logistic Regression

```python
from sklearn.linear_model import LogisticRegression

# Pipeline
pipeline_tfidf = Pipeline([
    ('tfidf', TfidfVectorizer(max_features=5000)),
    ('clf', LogisticRegression(max_iter=1000))
])

# Treinar e avaliar
pipeline_tfidf.fit(X_train, y_train)
acc_tfidf = pipeline_tfidf.score(X_test, y_test)

print(f"TF-IDF Accuracy: {acc_tfidf:.3f}")
# T√≠pico: ~87%
```

**Vantagens**:
- Simples e r√°pido
- N√£o precisa treinar embeddings
- Funciona bem para classifica√ß√£o

**Desvantagens**:
- N√£o captura sem√¢ntica
- Alta dimensionalidade

---

### Experimento 2: Word2Vec Pr√≥prio + Logistic Regression

```python
from gensim.models import Word2Vec
import numpy as np

# 1. Treinar Word2Vec
tokenized = [text.split() for text in X_train]
w2v = Word2Vec(
    sentences=tokenized,
    vector_size=100,
    window=5,
    min_count=2,
    workers=4
)

# 2. Vetorizar documentos (m√©dia dos vetores)
def doc_vector(text, model):
    words = text.split()
    vectors = [model.wv[w] for w in words if w in model.wv]
    if not vectors:
        return np.zeros(model.vector_size)
    return np.mean(vectors, axis=0)

X_train_w2v = np.array([doc_vector(t, w2v) for t in X_train])
X_test_w2v = np.array([doc_vector(t, w2v) for t in X_test])

# 3. Treinar
clf_w2v = LogisticRegression(max_iter=1000)
clf_w2v.fit(X_train_w2v, y_train)

acc_w2v = clf_w2v.score(X_test_w2v, y_test)
print(f"Word2Vec Accuracy: {acc_w2v:.3f}")
# T√≠pico: ~85% (menor que TF-IDF se corpus pequeno!)
```

**Vantagens**:
- Captura sem√¢ntica
- Dimensionalidade baixa (100D)

**Desvantagens**:
- Precisa corpus grande (>1M palavras)
- Perda de informa√ß√£o (m√©dia dos vetores)
- Pode ser pior que TF-IDF em datasets pequenos

---

### Experimento 3: Embeddings Pr√©-treinados + Logistic Regression

```python
from gensim.models import KeyedVectors

# 1. Carregar embeddings pr√©-treinados
pretrained = KeyedVectors.load_word2vec_format(
    'nilc_skip_s300.txt',  # 300 dimens√µes
    binary=False
)

# 2. Vetorizar
def doc_vector_pretrained(text, model):
    words = text.split()
    vectors = [model[w] for w in words if w in model]
    if not vectors:
        return np.zeros(300)
    return np.mean(vectors, axis=0)

X_train_pre = np.array([doc_vector_pretrained(t, pretrained) for t in X_train])
X_test_pre = np.array([doc_vector_pretrained(t, pretrained) for t in X_test])

# 3. Treinar
clf_pre = LogisticRegression(max_iter=1000)
clf_pre.fit(X_train_pre, y_train)

acc_pre = clf_pre.score(X_test_pre, y_test)
print(f"Pretrained Accuracy: {acc_pre:.3f}")
# T√≠pico: ~90% (melhor!)
```

**Vantagens**:
- N√£o precisa treinar embeddings
- Treinado em bilh√µes de palavras
- Captura sem√¢ntica
- **Geralmente o melhor para datasets pequenos-m√©dios**

**Desvantagens**:
- Arquivo grande (~1-5 GB)
- Dom√≠nio geral (n√£o espec√≠fico)

---

### Compara√ß√£o Final

```python
import matplotlib.pyplot as plt

resultados = {
    'TF-IDF': acc_tfidf,
    'Word2Vec\n(pr√≥prio)': acc_w2v,
    'Pr√©-treinado': acc_pre
}

plt.figure(figsize=(10, 6))
bars = plt.bar(resultados.keys(), resultados.values(), 
               color=['#3498db', '#e74c3c', '#2ecc71'])
plt.ylabel('Accuracy')
plt.title('Compara√ß√£o de Representa√ß√µes de Texto')
plt.ylim(0.5, 1.0)
plt.axhline(y=0.9, color='gray', linestyle='--', label='90% threshold')
plt.legend()

for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height,
             f'{height:.2%}', ha='center', va='bottom')

plt.show()
```

**Resultado T√≠pico**:
```
TF-IDF:          87% ‚≠ê‚≠ê‚≠ê
Word2Vec pr√≥prio: 85% ‚≠ê‚≠ê
Pr√©-treinado:     90% ‚≠ê‚≠ê‚≠ê‚≠ê ‚Üê Melhor!
```

---

### Quando Usar Cada Abordagem

| Abordagem | Dataset | Accuracy | Velocidade | Mem√≥ria |
|-----------|---------|----------|------------|---------|
| **TF-IDF** | Qualquer | 87% | ‚ö°‚ö°‚ö° | Baixa |
| **Word2Vec Pr√≥prio** | >100k docs | 85% | ‚ö° | Baixa |
| **Pr√©-treinado** | <10k docs | 90% | ‚ö°‚ö° | Alta (2GB+) |

**Regra pr√°tica**:
```python
if dataset_size < 10000:
    use_pretrained_embeddings()  # Melhor accuracy
elif dataset_size < 100000:
    use_tfidf()  # Simples e eficaz
else:
    train_word2vec()  # Aprende dom√≠nio espec√≠fico
```

---

## C√ìDIGO DE REFER√äNCIA COMPLETO

### Template Produ√ß√£o - Classifica√ß√£o de Texto

```python
# ===== IMPORTS =====
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import joblib

# ===== CARREGAR DADOS =====
df = pd.read_csv('dados.csv')
df.dropna(subset=['texto', 'categoria'], inplace=True)

X = df['texto']
y = df['categoria']

# ===== SPLIT =====
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ===== PIPELINE =====
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(
        max_features=10000,
        ngram_range=(1, 2),
        min_df=2,
        max_df=0.8,
        sublinear_tf=True
    )),
    ('clf', LinearSVC(
        C=1.0,
        max_iter=1000,
        random_state=42
    ))
])

# ===== GRID SEARCH (opcional) =====
param_grid = {
    'tfidf__max_features': [5000, 10000],
    'tfidf__ngram_range': [(1, 1), (1, 2)],
    'clf__C': [0.1, 1.0, 10.0]
}

grid = GridSearchCV(
    pipeline,
    param_grid,
    cv=5,
    n_jobs=-1,
    verbose=1,
    scoring='accuracy'
)

# ===== TREINAR =====
print("Treinando...")
grid.fit(X_train, y_train)

print(f"\nMelhores par√¢metros: {grid.best_params_}")
print(f"Melhor CV score: {grid.best_score_:.3f}")

# ===== AVALIAR =====
y_pred = grid.predict(X_test)

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# ===== MATRIZ DE CONFUS√ÉO =====
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=grid.classes_,
            yticklabels=grid.classes_)
plt.title('Matriz de Confus√£o')
plt.ylabel('Real')
plt.xlabel('Predito')
plt.tight_layout()
plt.savefig('confusion_matrix.png')
plt.show()

# ===== SALVAR MODELO =====
joblib.dump(grid.best_estimator_, 'modelo_texto.pkl')
print("\nModelo salvo em 'modelo_texto.pkl'")

# ===== USAR MODELO =====
model = joblib.load('modelo_texto.pkl')
novo_texto = "Texto de exemplo para classificar"
predicao = model.predict([novo_texto])
print(f"\nPredi√ß√£o: {predicao[0]}")
```

**Este c√≥digo √© pronto para produ√ß√£o!**

---

## HIPERPAR√ÇMETROS RECOMENDADOS

### TF-IDF
```python
TfidfVectorizer(
    max_features=10000,      # 10k para maioria dos casos
    ngram_range=(1, 2),      # Uni + Bigramas
    min_df=2,                # Ignora palavras em <2 docs
    max_df=0.8,              # Ignora palavras em >80% docs
    sublinear_tf=True,       # log(TF) em vez de TF
    strip_accents='unicode', # Remove acentos
    lowercase=True,          # Min√∫sculas
    stop_words=None          # N√£o remove (TF-IDF j√° filtra)
)
```

### Multinomial NB
```python
MultinomialNB(
    alpha=1.0,          # Laplace smoothing padr√£o
    fit_prior=True      # Aprende prior das classes
)
```

### LinearSVC
```python
LinearSVC(
    C=1.0,              # Regulariza√ß√£o padr√£o
    max_iter=1000,      # Suficiente para maioria
    dual=False,         # Primal se n_samples > n_features
    class_weight=None,  # Ou 'balanced' se desbalanceado
    random_state=42
)
```

### Logistic Regression
```python
LogisticRegression(
    C=1.0,                  # Regulariza√ß√£o inversa
    max_iter=1000,
    solver='lbfgs',         # Padr√£o e eficiente
    multi_class='ovr',      # One-vs-Rest
    class_weight=None,      # Ou 'balanced'
    random_state=42
)
```

---

## CHECKLIST DE CONCLUS√ÉO

- [ ] Treinei Naive Bayes
- [ ] Treinei LinearSVC
- [ ] Comparei TF-IDF vs Embeddings
- [ ] Sei escolher modelo por tarefa
- [ ] Entendo hiperpar√¢metros principais
- [ ] Criei pipeline completo pronto para produ√ß√£o

---

## TAGS DE BUSCA

`#modelos-classicos` `#naive-bayes` `#svm` `#logistic-regression` `#tfidf` `#word2vec` `#embeddings` `#classificacao-texto` `#sklearn` `#pipeline` `#grid-search`

---

**Vers√£o**: 1.0  
**Compatibilidade**: scikit-learn 1.0+  
**Uso recomendado**: Templates de produ√ß√£o, baseline r√°pido, compara√ß√£o de modelos
