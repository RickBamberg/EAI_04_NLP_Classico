# AGENT_CONTEXT.md - EAI_04 NLP Cl√°ssico (M√≥dulo Completo)

> **Prop√≥sito**: Vis√£o t√©cnica completa do m√≥dulo EAI_04  
> **√öltima atualiza√ß√£o**: Janeiro 2026  
> **Tipo**: M√≥dulo educacional com 3 se√ß√µes progressivas

## RESUMO EXECUTIVO

**Objetivo**: Ensinar NLP Cl√°ssico de forma pr√°tica e aplic√°vel  
**Estrutura**: Fundamentos ‚Üí Modelos ‚Üí Projetos  
**Notebooks**: 9 notebooks conceituais  
**Projetos**: 2 aplica√ß√µes Flask deployadas  
**T√©cnicas**: BoW, TF-IDF, Word2Vec, Sentence Transformers  
**Modelos**: Naive Bayes, SVM, Logistic Regression  
**Diferencial**: Progress√£o incremental com c√≥digo de produ√ß√£o

---

## DESIGN PEDAG√ìGICO

### Abordagem: Learning by Doing

```
Teoria ‚Üí Pr√°tica Guiada ‚Üí Aplica√ß√£o Aut√¥noma

Fundamentos (70% teoria, 30% pr√°tica)
    ‚Üì Construir base s√≥lida
Modelos Base (30% teoria, 70% pr√°tica)
    ‚Üì Templates prontos
Projetos (10% teoria, 90% pr√°tica)
    ‚Üì Aplica√ß√µes reais
```

### Progress√£o de Complexidade

| Se√ß√£o | Conceitos | C√≥digo | Autonomia | Objetivo |
|-------|-----------|--------|-----------|----------|
| **Fundamentos** | ‚≠ê‚≠ê‚≠ê | ‚≠ê | Guiado | Aprender |
| **Modelos Base** | ‚≠ê | ‚≠ê‚≠ê‚≠ê | Semi-guiado | Aplicar |
| **Projetos** | ‚≠ê | ‚≠ê‚≠ê‚≠ê | Aut√¥nomo | Deployar |

---

## SE√á√ÉO 1: FUNDAMENTOS

### Estrutura Pedag√≥gica

**Objetivo**: Base s√≥lida antes de modelos complexos  
**M√©todo**: Conceito ‚Üí C√≥digo ‚Üí Exemplo ‚Üí Exerc√≠cio

### Notebooks e Objetivos

#### 1. pre_processamento_texto.ipynb
**Objetivo de Aprendizado**: Pipeline de limpeza  
**Conceitos**:
```python
lowercase ‚Üí remove_punct ‚Üí tokenize ‚Üí 
remove_stopwords ‚Üí stemming ‚Üí clean_text
```

**C√≥digo Chave**:
```python
def preprocessar_texto(texto):
    texto = texto.lower()
    texto = re.sub(r'[^\w\s]', '', texto)
    palavras = texto.split()
    palavras = [p for p in palavras if p not in stopwords_pt]
    palavras = [stemmer.stem(p) for p in palavras]
    return ' '.join(palavras)
```

**Conceito-Chave**: "Garbage in, garbage out"

---

#### 2. bow_tfidf.ipynb
**Objetivo de Aprendizado**: Primeira representa√ß√£o vetorial  

**F√≥rmulas Matem√°ticas**:
```
BoW: word_count(w, d)

TF(w,d) = count(w,d) / total_words(d)
IDF(w) = log(N / df(w))
TF-IDF(w,d) = TF(w,d) √ó IDF(w)
```

**C√≥digo Chave**:
```python
# BoW
vectorizer_bow = CountVectorizer()
X_bow = vectorizer_bow.fit_transform(textos)

# TF-IDF
vectorizer_tfidf = TfidfVectorizer()
X_tfidf = vectorizer_tfidf.fit_transform(textos)
```

---

#### 3. representacao_bow_tfidf.ipynb
**Objetivo de Aprendizado**: Otimiza√ß√£o de par√¢metros  

**Par√¢metros Cr√≠ticos**:
```python
TfidfVectorizer(
    ngram_range=(1,2),  # Captura "n√£o gostei"
    max_df=0.8,         # Remove palavras muito comuns
    min_df=2,           # Remove typos
    max_features=10000  # Controla dimensionalidade
)
```

---

#### 4. word_embeddings.ipynb
**Objetivo de Aprendizado**: Representa√ß√£o sem√¢ntica  

**Arquiteturas**:
```
CBOW: Contexto ‚Üí Palavra
Skip-gram: Palavra ‚Üí Contexto
```

**C√≥digo Chave**:
```python
from gensim.models import Word2Vec

model = Word2Vec(
    sentences=corpus_tokenizado,
    vector_size=100,
    window=5,
    sg=0  # 0=CBOW, 1=Skip-gram
)

# Sem√¢ntica
model.wv.most_similar('rei')
# [('rainha', 0.89), ...]
```

---

#### 5. pretrained_embeddings.ipynb
**Objetivo de Aprendizado**: Usar embeddings prontos  

**Vantagens**:
- ‚úÖ N√£o precisa treinar
- ‚úÖ Bilh√µes de palavras
- ‚úÖ Alta qualidade

**C√≥digo Chave**:
```python
from gensim.models import KeyedVectors

model = KeyedVectors.load_word2vec_format(
    'skip_s300.txt',
    binary=False
)
```

---

#### 6. analise_sentimentos.ipynb
**Objetivo de Aprendizado**: Pipeline end-to-end  

**Pipeline Completo**:
```python
Pipeline([
    ('tfidf', TfidfVectorizer()),
    ('clf', LogisticRegression())
])
```

**Performance Esperada**: ~85-90% accuracy

---

## SE√á√ÉO 2: MODELOS BASE

### Estrutura Pedag√≥gica

**Objetivo**: Templates de c√≥digo prontos para produ√ß√£o  
**M√©todo**: Modelo ‚Üí Hiperpar√¢metros ‚Üí Uso ‚Üí Quando usar

### Notebooks e Arquiteturas

#### 1. naive_bayes_sentimentos.ipynb

**Modelo**: MultinomialNB  

**Por que funciona em NLP**:
```
P(classe|doc) ‚àù P(classe) √ó ‚àè P(palavra|classe)

Assumption "naive": palavras independentes
Simplifica c√°lculo MUITO!
```

**Hiperpar√¢metros**:
```python
MultinomialNB(
    alpha=1.0  # Laplace smoothing
)

# alpha=0.1: Menos smoothing
# alpha=1.0: Padr√£o ‚úì
# alpha=10.0: Mais smoothing
```

**Performance**: ~85-88%  
**Quando usar**: Baseline, poucos dados

---

#### 2. classificacao_texto_svm.ipynb

**Modelo**: LinearSVC  

**Por que funciona em NLP**:
```
Encontra hiperplano com margem m√°xima
Funciona bem em alta dimensionalidade (10k-100k features)
```

**Hiperpar√¢metros**:
```python
LinearSVC(
    C=1.0  # Regulariza√ß√£o inversa
)

# C=0.1: Mais regulariza√ß√£o
# C=1.0: Padr√£o ‚úì
# C=10.0: Menos regulariza√ß√£o
```

**Performance**: ~89-92%  
**Quando usar**: Melhor accuracy, produ√ß√£o

---

#### 3. comparativo_tfidf_vs_embeddings.ipynb

**Experimento**: 3 abordagens  

**Resultados T√≠picos**:
```python
TF-IDF + LogReg:              87%
Word2Vec pr√≥prio + LogReg:    85%
Embeddings pr√©-treinados:     90% ‚Üê Melhor!
```

**Li√ß√£o**: Pr√©-treinados > TF-IDF para datasets pequenos

---

## SE√á√ÉO 3: PROJETOS

### Estrutura Pedag√≥gica

**Objetivo**: C√≥digo de produ√ß√£o deployado  
**M√©todo**: Arquitetura ‚Üí Implementa√ß√£o ‚Üí Deploy ‚Üí Uso

### Projeto 1: An√°lise de Feedback

#### Arquitetura

```
Sistema de Dupla Classifica√ß√£o:

Input: "Adorei! Sugiro adicionar mais cores"
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                       ‚îÇ
Modelo 1           Modelo 2
Sentimento         Sugest√£o
    ‚Üì                  ‚Üì
Positivo (95%)     Sim (97%)
    ‚Üì                  ‚Üì
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚Üì
    Output Combinado
```

#### Tecnologia

**Modelo 1 - Sentimento**:
```python
Pipeline([
    ('tfidf', TfidfVectorizer(
        ngram_range=(1,2),
        max_features=50000
    )),
    ('clf', LogisticRegression())
])

Dataset: B2W-Reviews (113k)
Performance: 95% accuracy
```

**Modelo 2 - Sugest√£o**:
```python
# Mesma arquitetura
Dataset: Sugest√µes IA + B2W filtrado (3k balanceado)
Performance: 98% accuracy
```

#### Conceito-Chave

**Por que 2 modelos?**
```python
# Abordagem 1 (n√£o usada): Multi-label
classes = [
    'Pos+Sug', 'Pos+NoSug',
    'Neg+Sug', 'Neg+NoSug'
]
# Complexo, requer mais dados

# Abordagem 2 (usada): 2 bin√°rios ‚úì
# Simples, modular, melhor performance
```

---

### Projeto 2: Sistema de Busca FAQs

#### Arquitetura

```
Busca Sem√¢ntica:

Pergunta Usu√°rio: "Como fazer PIX?"
    ‚Üì
Sentence Transformer (embedding 512D)
    ‚Üì
Similaridade Cosseno com Base (1172 FAQs)
    ‚Üì
Top 3 Resultados (‚â•50% threshold)
    ‚Üì
1. "Como acesso o PIX?" (87%)
2. "Como cadastrar chave?" (74%)
3. "Qual limite PIX?" (62%)
```

#### Tecnologia

**Modelo**:
```python
SentenceTransformer('distiluse-base-multilingual-cased-v1')
# 512 dimens√µes
# Multil√≠ngue
# Distilado (r√°pido)
```

**Busca**:
```python
from sklearn.metrics.pairwise import cosine_similarity

sims = cosine_similarity(
    embedding_query,
    embeddings_base
)

# Filtrar por threshold
resultados = [r for r in top_k if sim >= 0.5]
```

#### Conceito-Chave

**Busca Sem√¢ntica vs Keywords**:
```
Keywords: "fazer" ‚â† "realizar" ‚Üí Miss
Sem√¢ntica: Entende sin√¥nimos ‚Üí Hit (87%)
```

---

## COMPARA√á√ÉO DE T√âCNICAS

### Representa√ß√µes

| T√©cnica | Dim | Tipo | Sem√¢ntica | Treino | Uso T√≠pico |
|---------|-----|------|-----------|--------|------------|
| **BoW** | 10k-100k | Esparso | ‚ùå | N√£o | Baseline |
| **TF-IDF** | 10k-100k | Esparso | ‚ùå | N√£o | Classifica√ß√£o |
| **Word2Vec** | 100-300 | Denso | ‚úÖ | Sim | Similaridade |
| **FastText** | 100-300 | Denso | ‚úÖ | Sim | OOV words |
| **Sentence Transformers** | 512-768 | Denso | ‚úÖ‚úÖ | N√£o | Busca sem√¢ntica |

### Modelos

| Modelo | Accuracy | Treino | Predi√ß√£o | Interpret√°vel | Produ√ß√£o |
|--------|----------|--------|----------|---------------|----------|
| **Naive Bayes** | 85% | ‚ö°‚ö°‚ö° | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | ‚úÖ |
| **Logistic Reg** | 87% | ‚ö°‚ö° | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | ‚úÖ |
| **LinearSVC** | 90% | ‚ö° | ‚ö°‚ö° | ‚≠ê‚≠ê | ‚úÖ |
| **Random Forest** | 88% | üê¢ | üê¢ | ‚≠ê | ‚ùå |

---

## PIPELINE T√çPICO

### Template de Produ√ß√£o

```python
# 1. Imports
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report
import joblib

# 2. Dados
df = pd.read_csv('dados.csv')
X = df['texto']
y = df['categoria']

# 3. Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y
)

# 4. Pipeline
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(
        max_features=10000,
        ngram_range=(1,2)
    )),
    ('clf', LinearSVC(C=1.0))
])

# 5. Treinar
pipeline.fit(X_train, y_train)

# 6. Avaliar
print(classification_report(y_test, pipeline.predict(X_test)))

# 7. Salvar
joblib.dump(pipeline, 'modelo.pkl')
```

**Este pipeline resolve 80% dos problemas!**

---

## DEPLOYMENT

### Flask B√°sico

```python
from flask import Flask, request
import joblib

app = Flask(__name__)
model = joblib.load('modelo.pkl')

@app.route('/predict', methods=['POST'])
def predict():
    texto = request.json['texto']
    pred = model.predict([texto])[0]
    return {'predicao': pred}

if __name__ == '__main__':
    app.run()
```

### Docker

```dockerfile
FROM python:3.9-slim
COPY . /app
WORKDIR /app
RUN pip install -r requirements.txt
CMD ["python", "app.py"]
```

### Heroku

```bash
# Procfile
web: gunicorn app:app

# Deploy
heroku create
git push heroku main
```

---

## TROUBLESHOOTING COMUM

### Problema 1: Accuracy Baixa (<70%)

```python
# Causas:
1. Pr√©-processamento ruim ‚Üí Verificar stopwords
2. Dados desbalanceados ‚Üí class_weight='balanced'
3. Features insuficientes ‚Üí Aumentar max_features
4. Modelo inadequado ‚Üí Testar SVM em vez de NB
```

### Problema 2: Modelo N√£o Converge

```python
# Solu√ß√£o:
LinearSVC(max_iter=2000)  # Aumentar itera√ß√µes
```

### Problema 3: OOM (Out of Memory)

```python
# Solu√ß√£o:
TfidfVectorizer(max_features=5000)  # Reduzir features
```

---

## PROGRESS√ÉO PARA NLP MODERNO

### Funda√ß√£o (EAI_04 - Este M√≥dulo)
```
‚úÖ TF-IDF
‚úÖ Word2Vec
‚úÖ Naive Bayes, SVM
‚úÖ Sentence Transformers (b√°sico)
```

### Pr√≥ximo N√≠vel (EAI_05)
```
‚Üí Transformers (aten√ß√£o)
‚Üí BERT, RoBERTa
‚Üí Fine-tuning
‚Üí GPT (generativo)
‚Üí Hugging Face
```

**Base S√≥lida**: Este m√≥dulo √© essencial para entender Transformers!

---

## M√âTRICAS DE SUCESSO

### Conhecimento
- [ ] Entende TF-IDF matematicamente
- [ ] Sabe quando usar BoW vs Embeddings
- [ ] Domina pr√©-processamento
- [ ] Conhece limita√ß√µes de cada t√©cnica

### Habilidades
- [ ] Treina modelo do zero
- [ ] Cria pipeline completo
- [ ] Compara modelos
- [ ] Usa embeddings pr√©-treinados

### Aplica√ß√£o
- [ ] Executou 2 projetos
- [ ] Modificou projeto existente
- [ ] Criou projeto pr√≥prio
- [ ] Fez deploy em produ√ß√£o

---

## BIBLIOTECAS DO M√ìDULO

```python
# NLP Cl√°ssico
nltk              # Stopwords, stemming
scikit-learn      # TF-IDF, modelos ML
gensim            # Word2Vec, FastText
sentence-transformers  # Embeddings de senten√ßas

# Dados
pandas            # DataFrames
numpy             # Arrays
beautifulsoup4    # Limpeza HTML

# Web
flask             # Deployment

# Visualiza√ß√£o
matplotlib        # Gr√°ficos
seaborn           # Heatmaps
```

---

## DATASETS UTILIZADOS

```python
# Fundamentos
'noticias_sinteticas.csv'  # Pequeno, did√°tico

# Projetos
'B2W-Reviews01.csv'        # 129k reviews PT-BR
'sugestoes.txt'            # 1.5k sugest√µes IA
'FAQ_BB.json'              # 1.2k FAQs Banco Central
```

---

## ESTAT√çSTICAS DO M√ìDULO

**Documenta√ß√£o**:
- 12 arquivos markdown
- ~5.000 linhas de c√≥digo
- ~120.000 palavras

**C√≥digo**:
- 9 notebooks
- 2 aplica√ß√µes Flask
- 200+ snippets reutiliz√°veis

**Tempo**:
- Trilha completa: 3-4 semanas
- Trilha r√°pida: 1 semana

---

## TAGS DE BUSCA

`#nlp-classico` `#tfidf` `#word2vec` `#naive-bayes` `#svm` `#sentence-transformers` `#classificacao-texto` `#analise-sentimento` `#busca-semantica` `#sklearn` `#nltk` `#portuguese-nlp` `#flask` `#deployment`

---

**Vers√£o**: 1.0  
**Compatibilidade**: Python 3.7+  
**Uso recomendado**: Aprendizado incremental, baseline r√°pido, produ√ß√£o leve
