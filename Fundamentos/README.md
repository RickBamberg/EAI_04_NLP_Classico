# Fundamentos de NLP ClÃ¡ssico

## ðŸ“Œ Sobre

Esta pasta contÃ©m **notebooks fundamentais** que explicam as tÃ©cnicas clÃ¡ssicas de Processamento de Linguagem Natural (NLP), desde prÃ©-processamento de texto atÃ© representaÃ§Ãµes vetoriais (BoW, TF-IDF, Word Embeddings).

**Objetivo**: Fornecer base sÃ³lida em NLP clÃ¡ssico antes de partir para modelos modernos (Transformers, BERT).

---

## ðŸŽ¯ Por Que NLP ClÃ¡ssico?

Mesmo com modelos modernos como BERT e GPT, tÃ©cnicas clÃ¡ssicas sÃ£o essenciais:
- âœ… **Baseline rÃ¡pido**: TF-IDF + SVM funciona bem em muitos casos
- âœ… **EficiÃªncia**: Menor custo computacional
- âœ… **Interpretabilidade**: Mais fÃ¡cil entender o que o modelo aprendeu
- âœ… **ProduÃ§Ã£o**: Modelos menores e mais rÃ¡pidos para deploy

---

## ðŸ“‚ Notebooks DisponÃ­veis

### 1ï¸âƒ£ **pre_processamento_texto.ipynb** (FundaÃ§Ã£o)

**TÃ³picos**:
- Limpeza de texto (lowercase, remoÃ§Ã£o de pontuaÃ§Ã£o)
- RemoÃ§Ã£o de stopwords (palavras comuns)
- TokenizaÃ§Ã£o (quebrar texto em palavras)
- Stemming vs Lemmatization
- NormalizaÃ§Ã£o (acentos, espaÃ§os)

**TÃ©cnicas**:
```python
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import RSLPStemmer

# Remover pontuaÃ§Ã£o
texto = re.sub(r'[^\w\s]', '', texto)

# Remover stopwords
stopwords_pt = set(stopwords.words('portuguese'))
palavras = [p for p in palavras if p not in stopwords_pt]

# Stemming
stemmer = RSLPStemmer()
palavras_stem = [stemmer.stem(p) for p in palavras]
```

**Para Quem**: Todos - Ã© a base de qualquer projeto NLP

**DuraÃ§Ã£o**: ~20 minutos

---

### 2ï¸âƒ£ **bow_tfidf.ipynb** (RepresentaÃ§Ãµes BÃ¡sicas)

**TÃ³picos**:

#### Bag of Words (BoW)
- Conta frequÃªncia de cada palavra
- Ignora ordem e contexto
- Vetor esparso (muitos zeros)

```python
from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer()
X_bow = vectorizer.fit_transform(textos)

# Exemplo: "gato" aparece 3 vezes â†’ valor = 3
```

#### TF-IDF (Term Frequency - Inverse Document Frequency)
- **TF**: FrequÃªncia do termo no documento
- **IDF**: ImportÃ¢ncia do termo no corpus
- Penaliza palavras muito comuns

```python
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
X_tfidf = vectorizer.fit_transform(textos)

# Palavras raras tÃªm peso maior
# Palavras comuns tÃªm peso menor
```

**FÃ³rmulas**:
```
TF(t,d) = (nÃºmero de vezes que t aparece em d) / (total de termos em d)

IDF(t) = log(N / df(t))
  onde N = total de documentos
       df(t) = documentos que contÃªm t

TF-IDF(t,d) = TF(t,d) Ã— IDF(t)
```

**ComparaÃ§Ã£o**:
| Aspecto | BoW | TF-IDF |
|---------|-----|--------|
| **Pesos** | Contagem simples | ImportÃ¢ncia relativa |
| **Palavras comuns** | Peso alto | Peso baixo |
| **Palavras raras** | Peso baixo | Peso alto |
| **Uso** | Baseline | ClassificaÃ§Ã£o, busca |

**Para Quem**: Iniciantes em NLP

**DuraÃ§Ã£o**: ~30 minutos

---

### 3ï¸âƒ£ **representacao_bow_tfidf.ipynb** (Aprofundamento)

**TÃ³picos**:
- VariaÃ§Ãµes de BoW (n-gramas)
- ParÃ¢metros do TF-IDF (max_df, min_df)
- NormalizaÃ§Ã£o L2
- AnÃ¡lise de dimensionalidade

**N-gramas**:
```python
# Unigramas: ["bom", "filme"]
# Bigramas: ["bom filme"]
# Trigramas: ["muito bom filme"]

vectorizer = TfidfVectorizer(ngram_range=(1,2))  # Uni + Bigramas
```

**Filtros**:
```python
vectorizer = TfidfVectorizer(
    max_df=0.8,  # Ignora palavras em >80% dos docs
    min_df=2,    # Ignora palavras em <2 docs
    max_features=1000  # Top 1000 features
)
```

**Para Quem**: IntermediÃ¡rio

**DuraÃ§Ã£o**: ~40 minutos

---

### 4ï¸âƒ£ **word_embeddings.ipynb** (RepresentaÃ§Ãµes Densas)

**TÃ³picos**:

#### Word2Vec
- Vetores densos (ex: 100 dimensÃµes)
- Captura semÃ¢ntica: "rei" - "homem" + "mulher" â‰ˆ "rainha"
- 2 arquiteturas: CBOW e Skip-gram

```python
from gensim.models import Word2Vec

# Treinar Word2Vec
model = Word2Vec(
    sentences=corpus_tokenizado,
    vector_size=100,
    window=5,
    min_count=2,
    workers=4
)

# Obter vetor de uma palavra
vetor_rei = model.wv['rei']

# Palavras similares
similares = model.wv.most_similar('rei', topn=5)
```

#### FastText
- ExtensÃ£o do Word2Vec
- Usa subpalavras (character n-grams)
- Funciona com palavras fora do vocabulÃ¡rio

```python
from gensim.models import FastText

model = FastText(
    sentences=corpus_tokenizado,
    vector_size=100,
    window=5,
    min_count=2
)

# Funciona mesmo para palavras novas!
vetor_palavra_nova = model.wv['palavrainexistente123']
```

**BoW/TF-IDF vs Word Embeddings**:
| Aspecto | BoW/TF-IDF | Word Embeddings |
|---------|------------|-----------------|
| **Dimensionalidade** | Alta (10k-100k) | Baixa (50-300) |
| **Esparsidade** | Esparso | Denso |
| **SemÃ¢ntica** | NÃ£o captura | Captura |
| **OOV** | Ignora | FastText funciona |

**Para Quem**: IntermediÃ¡rio a avanÃ§ado

**DuraÃ§Ã£o**: ~1 hora

---

### 5ï¸âƒ£ **pretrained_embeddings.ipynb** (Embeddings PrÃ©-treinados)

**TÃ³picos**:
- Carregar Word2Vec prÃ©-treinado (Google, NILC)
- GloVe embeddings
- Como usar em modelos

**Word2Vec NILC (PortuguÃªs)**:
```python
from gensim.models import KeyedVectors

# Carregar modelo prÃ©-treinado
model = KeyedVectors.load_word2vec_format(
    'skip_s300.txt',
    binary=False
)

# Usar vetores
vetor = model['computador']
similares = model.most_similar('computador', topn=10)
```

**Vantagens**:
- âœ… NÃ£o precisa treinar (economia de tempo)
- âœ… Treinado em corpus gigante (melhor qualidade)
- âœ… Funciona bem com poucos dados

**Fontes**:
- [NILC Word2Vec](http://nilc.icmc.usp.br/embeddings)
- [GloVe](https://nlp.stanford.edu/projects/glove/)
- [FastText Facebook](https://fasttext.cc/docs/en/crawl-vectors.html)

**Para Quem**: Quem quer pular treino de embeddings

**DuraÃ§Ã£o**: ~30 minutos

---

### 6ï¸âƒ£ **analise_sentimentos.ipynb** (AplicaÃ§Ã£o PrÃ¡tica)

**TÃ³picos**:
- ClassificaÃ§Ã£o de sentimento (positivo/negativo)
- Dataset de reviews
- Pipeline completo: PrÃ©-processamento â†’ TF-IDF â†’ Modelo

**Pipeline**:
```python
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

# Pipeline end-to-end
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(max_features=5000)),
    ('clf', LogisticRegression())
])

# Treinar
pipeline.fit(X_train, y_train)

# Prever
sentimento = pipeline.predict(["Adorei o filme!"])
# Output: 'positivo'
```

**Dataset TÃ­pico**:
```
Texto: "Filme excelente, recomendo!"
Sentimento: positivo

Texto: "PÃ©ssimo, perdi meu tempo"
Sentimento: negativo
```

**MÃ©tricas**:
- Accuracy: ~85-90%
- Precision/Recall por classe

**Para Quem**: Todos - aplicaÃ§Ã£o prÃ¡tica imediata

**DuraÃ§Ã£o**: ~45 minutos

---

## ðŸ—ºï¸ Ordem de Estudo Recomendada

### Iniciante (Nunca viu NLP)
```
1. pre_processamento_texto.ipynb      (base essencial)
2. bow_tfidf.ipynb                     (primeira representaÃ§Ã£o)
3. analise_sentimentos.ipynb           (aplicaÃ§Ã£o prÃ¡tica)
4. word_embeddings.ipynb               (representaÃ§Ã£o avanÃ§ada)
5. pretrained_embeddings.ipynb         (usar embeddings prontos)
6. representacao_bow_tfidf.ipynb       (aprofundamento)
```

### IntermediÃ¡rio (JÃ¡ conhece ML)
```
1. pre_processamento_texto.ipynb      (revisÃ£o rÃ¡pida)
2. bow_tfidf.ipynb                     (conceitos)
3. word_embeddings.ipynb               (foco aqui)
4. analise_sentimentos.ipynb           (aplicaÃ§Ã£o)
```

### AvanÃ§ado (RevisÃ£o RÃ¡pida)
```
1. word_embeddings.ipynb               (conceitos chave)
2. pretrained_embeddings.ipynb         (uso prÃ¡tico)
3. Pular para Modelos_Base/
```

---

## ðŸ“Š ComparaÃ§Ã£o de TÃ©cnicas

### Quando Usar Cada RepresentaÃ§Ã£o?

| TÃ©cnica | Quando Usar | Vantagens | Desvantagens |
|---------|-------------|-----------|--------------|
| **BoW** | Baseline rÃ¡pido, poucos dados | Simples, rÃ¡pido | Ignora ordem, sem semÃ¢ntica |
| **TF-IDF** | ClassificaÃ§Ã£o, busca textual | Filtra palavras comuns | Ainda sem semÃ¢ntica |
| **Word2Vec** | SemÃ¢ntica importa, corpus mÃ©dio | Captura relaÃ§Ãµes | Precisa treinar |
| **FastText** | Palavras fora vocabulÃ¡rio | Funciona com OOV | Mais lento |
| **PrÃ©-treinados** | Poucos dados, produÃ§Ã£o | Qualidade alta, rÃ¡pido | Tamanho do arquivo |

---

## ðŸ”‘ Conceitos-Chave

### PrÃ©-processamento

**Stopwords**: Palavras muito comuns sem valor semÃ¢ntico
```python
# PortuguÃªs
stopwords = ['a', 'o', 'de', 'da', 'em', 'para', 'com', ...]
```

**Stemming vs Lemmatization**:
```python
# Stemming (regras heurÃ­sticas)
correr â†’ corr
corrida â†’ corr
correndo â†’ corr

# Lemmatization (anÃ¡lise linguÃ­stica)
correr â†’ correr
corrida â†’ corrida
correndo â†’ correr
```

### VetorizaÃ§Ã£o

**Esparso vs Denso**:
```python
# BoW/TF-IDF: Esparso (muitos zeros)
[0, 0, 3, 0, 0, 1, 0, 0, ..., 0]  # 10.000 dimensÃµes

# Word2Vec: Denso
[0.23, -0.45, 0.67, ..., 0.12]  # 100 dimensÃµes
```

---

## ðŸ’» CÃ³digo Base ReutilizÃ¡vel

### Pipeline Completo de PrÃ©-processamento

```python
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import RSLPStemmer

nltk.download('stopwords')
stopwords_pt = set(stopwords.words('portuguese'))
stemmer = RSLPStemmer()

def preprocessar_texto(texto):
    """
    Pipeline completo de prÃ©-processamento
    """
    # 1. Lowercase
    texto = texto.lower()
    
    # 2. Remover pontuaÃ§Ã£o
    texto = re.sub(r'[^\w\s]', '', texto)
    
    # 3. Remover nÃºmeros
    texto = re.sub(r'\d+', '', texto)
    
    # 4. Tokenizar
    palavras = texto.split()
    
    # 5. Remover stopwords
    palavras = [p for p in palavras if p not in stopwords_pt]
    
    # 6. Stemming (opcional)
    palavras = [stemmer.stem(p) for p in palavras]
    
    # 7. Juntar
    return ' '.join(palavras)

# Uso
texto_limpo = preprocessar_texto("O filme foi excelente!")
```

### Pipeline Scikit-learn Completo

```python
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.model_selection import train_test_split

# Dados
X = ["texto 1", "texto 2", ...]
y = [0, 1, ...]

# Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Pipeline
pipeline = Pipeline([
    ('preprocessamento', FunctionTransformer(lambda x: [preprocessar_texto(t) for t in x])),
    ('tfidf', TfidfVectorizer(max_features=5000)),
    ('clf', LinearSVC())
])

# Treinar
pipeline.fit(X_train, y_train)

# Avaliar
accuracy = pipeline.score(X_test, y_test)
print(f"Accuracy: {accuracy:.2%}")
```

---

## ðŸŽ¯ Checklist de Aprendizado

### Conceitos Fundamentais
- [ ] Entendo diferenÃ§a entre BoW, TF-IDF e Word Embeddings
- [ ] Sei quando usar stemming vs lemmatization
- [ ] Compreendo stopwords e por que removÃª-las
- [ ] Entendo vetores esparsos vs densos

### TÃ©cnicas
- [ ] Sei aplicar TF-IDF
- [ ] Sei treinar Word2Vec
- [ ] Sei usar embeddings prÃ©-treinados
- [ ] Sei criar pipeline completo

### PrÃ¡tica
- [ ] Executei todos os 6 notebooks
- [ ] Apliquei em um dataset prÃ³prio
- [ ] Comparei BoW vs TF-IDF vs Word2Vec

---

## ðŸ“š Recursos Complementares

### Bibliotecas
- [NLTK](https://www.nltk.org/) - Ferramentas NLP clÃ¡ssicas
- [spaCy](https://spacy.io/) - NLP moderno e rÃ¡pido
- [Gensim](https://radimrehurek.com/gensim/) - Word2Vec, FastText

### Datasets
- [IMDB Reviews](http://ai.stanford.edu/~amaas/data/sentiment/) - AnÃ¡lise de sentimento
- [20 Newsgroups](http://qwone.com/~jason/20Newsgroups/) - ClassificaÃ§Ã£o de texto
- [B2W-Reviews](https://github.com/americanas-tech/b2w-reviews01) - Reviews em portuguÃªs

### Cursos
- [Coursera NLP Specialization](https://www.coursera.org/specializations/natural-language-processing)
- [Fast.ai NLP](https://www.fast.ai/)

---

## ðŸ”§ Troubleshooting

### Problema: "Resource stopwords not found"
**SoluÃ§Ã£o**:
```python
import nltk
nltk.download('stopwords')
nltk.download('rslp')  # Para stemmer portuguÃªs
```

### Problema: Vetores muito grandes (OOM)
**SoluÃ§Ã£o**:
```python
# Limitar features
vectorizer = TfidfVectorizer(max_features=5000)

# Ou usar vetores esparsos
from scipy.sparse import save_npz
save_npz('vetores.npz', X_sparse)
```

### Problema: Word2Vec lento
**SoluÃ§Ã£o**:
```python
# Usar workers
model = Word2Vec(sentences, workers=4)

# Ou usar embeddings prÃ©-treinados
```

---

## ðŸ’¡ Dicas de Estudo

1. **Execute cÃ©lula por cÃ©lula**
   - NÃ£o apenas leia, execute e observe

2. **Teste com seus textos**
   - Aplique em tweets, comentÃ¡rios, artigos

3. **Compare resultados**
   - BoW vs TF-IDF: Qual funciona melhor?

4. **Visualize embeddings**
   - Use t-SNE para ver agrupamentos

5. **Construa vocabulÃ¡rio**
   - Anote termos tÃ©cnicos (corpus, token, stem)

---

## ðŸš€ PrÃ³ximos Passos

ApÃ³s dominar os fundamentos:

1. **Ir para Modelos_Base/** - Modelos prontos (SVM, Naive Bayes)
2. **Ir para Projetos/** - AplicaÃ§Ãµes deployadas
3. **Explorar NLP Moderno** - Transformers, BERT (EAI_05)

---

**Lembre-se**: NLP clÃ¡ssico ainda Ã© MUITO usado em produÃ§Ã£o. Domine antes de partir para modelos complexos!

*Desenvolvido como parte do curso "Especialista em IA" - MÃ³dulo EAI_04*
